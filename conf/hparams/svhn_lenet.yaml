# @package _global_

dataset:
  batch_size: 32

model:
  num_classes: 10

hparams:
  epochs: 20
  max_grad_norm: null

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
    weight_decay: 1e-3

  scheduler:
    _target_: flpert.scheduler.ExponentialDecayLR
    decay_rate: 0.1
    decay_steps: 10000
    staircase: false